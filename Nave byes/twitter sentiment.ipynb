{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                   id      conversation_id    created_at  \\\n",
      "0           0  1371886393245495297  1371886393245495297  1.615918e+12   \n",
      "1           1  1371840814578548741  1371840814578548741  1.615907e+12   \n",
      "2           2  1371819206233104387  1371799495646736388  1.615902e+12   \n",
      "3           3  1371819165070225416  1371802619312701440  1.615902e+12   \n",
      "4           4  1371802898733035520  1371802898733035520  1.615898e+12   \n",
      "\n",
      "                  date  timezone place  \\\n",
      "0  2021-03-16 18:09:39         0   NaN   \n",
      "1  2021-03-16 15:08:32         0   NaN   \n",
      "2  2021-03-16 13:42:40         0   NaN   \n",
      "3  2021-03-16 13:42:30         0   NaN   \n",
      "4  2021-03-16 12:37:52         0   NaN   \n",
      "\n",
      "                                               tweet language hashtags  ...  \\\n",
      "0  Today visited Expo Center with my mother for h...       en       []  ...   \n",
      "1  @anjanaomkashyap  is it possible to get data n...       en       []  ...   \n",
      "2  @StayingReal0511 More alarmingly, over 12,000 ...       en       []  ...   \n",
      "3  @StayingReal0511 More alarmingly, over 12,000 ...       en       []  ...   \n",
      "4  My motherâ€™s b.p shot up to 180/90 for 25 days ...       en       []  ...   \n",
      "\n",
      "                     geo  source  user_rt_id user_rt retweet_id  \\\n",
      "0  28.7041,77.1025,400mi     NaN         NaN     NaN        NaN   \n",
      "1  28.7041,77.1025,400mi     NaN         NaN     NaN        NaN   \n",
      "2  28.7041,77.1025,400mi     NaN         NaN     NaN        NaN   \n",
      "3  28.7041,77.1025,400mi     NaN         NaN     NaN        NaN   \n",
      "4  28.7041,77.1025,400mi     NaN         NaN     NaN        NaN   \n",
      "\n",
      "                                            reply_to  retweet_date translate  \\\n",
      "0                                                 []           NaN       NaN   \n",
      "1                                                 []           NaN       NaN   \n",
      "2  [{'screen_name': 'StayingReal0511', 'name': 'S...           NaN       NaN   \n",
      "3  [{'screen_name': 'StayingReal0511', 'name': 'S...           NaN       NaN   \n",
      "4                                                 []           NaN       NaN   \n",
      "\n",
      "  trans_src trans_dest  \n",
      "0       NaN        NaN  \n",
      "1       NaN        NaN  \n",
      "2       NaN        NaN  \n",
      "3       NaN        NaN  \n",
      "4       NaN        NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import nltk\n",
    "data = pd.read_csv(\"tweet.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopword=set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\abdu\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"tweet\"]]\n",
    "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"tweet\"]]\n",
    "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  Positive  Negative  \\\n",
      "0  today visit expo center mother first dose vacc...     0.195     0.000   \n",
      "1  anjanaomkashyap  possibl get data n effect vac...     0.000     0.000   \n",
      "2   alarm  peopl receiv dose pfizerbiontech vacci...     0.000     0.211   \n",
      "3   alarm  peopl receiv dose pfizerbiontech vacci...     0.000     0.211   \n",
      "4  mother bp shot   day first dose covishield  br...     0.196     0.000   \n",
      "\n",
      "   Neutral  \n",
      "0    0.805  \n",
      "1    1.000  \n",
      "2    0.789  \n",
      "3    0.789  \n",
      "4    0.804  \n"
     ]
    }
   ],
   "source": [
    "data = data[[\"tweet\", \"Positive\", \"Negative\", \"Neutral\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral \n"
     ]
    }
   ],
   "source": [
    "x = sum(data[\"Positive\"])\n",
    "y = sum(data[\"Negative\"])\n",
    "z = sum(data[\"Neutral\"])\n",
    "\n",
    "def sentiment_score(a, b, c):\n",
    "    if (a>b) and (a>c):\n",
    "        print(\"Positive \")\n",
    "    elif (b>a) and (b>c):\n",
    "        print(\"Negative \")\n",
    "    else:\n",
    "        print(\"Neutral \")\n",
    "\n",
    "sentiment_score(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  243.6220000000002\n",
      "Negative:  82.49899999999985\n",
      "Neutral:  2697.881999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive: \", x)\n",
    "print(\"Negative: \", y)\n",
    "print(\"Neutral: \", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
